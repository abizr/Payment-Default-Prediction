# -*- coding: utf-8 -*-
"""Source_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UyyW_d2MlBXSL43WH2pYNGsQ6K-5yDTR
"""

from google.colab import drive
drive.mount("/content/gdrive")

"""# Import Library"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

from matplotlib import rcParams

rcParams['figure.figsize'] = 12,5
rcParams['lines.linewidth'] = 3
rcParams['xtick.labelsize'] = 'x-large'
rcParams['ytick.labelsize'] = 'x-large'

"""# Load Dataset"""

df = pd.read_csv('/content/gdrive/MyDrive/dataset/train_20D8GL3.csv')

#df = pd.read_csv('/content/gdrive/MyDrive/dataset/paymentdefault/train_20D8GL3.csv')

"""Dataset yang digunakan adalah data train untuk payment default prediction. Berikut link untuk mengakses datasetnya: https://www.kaggle.com/datasets/reverie5/av-janata-hack-payment-default-prediction?select=train_20D8GL3.csv

### Deskripsi Kolom

*   <b>ID</b> : Unique ID of each client
*   <b>LIMIT_BAL</b> : Amount of given credit (NT dollars): It includes both the individual consumer credit and his/her family
*   <b>SEX</b> : Gender (1=male, 2=female)
*   <b>EDUCATION</b> : 1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown
*   <b>MARRIAGE</b> : Marital status (1=married, 2=single, 3=divorced)
*   <b>AGE</b> : Age of the client
*   <b>PAY_0</b> : Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months) -2 = bayar tagihan penuh dan tidak ada tagihan dibulan tersebut 0 = bayar minimal dari total tagihan bulan tersebut
*   <b>PAY_2</b> : Repayment status in August, 2005 (scale same as above
*   <b>PAY_3</b> : Repayment status in July, 2005 (scale same as above)
*   <b>PAY_4</b> : Repayment status in June, 2005 (scale same as above)
*   <b>PAY_5 </b>: Repayment status in May, 2005 (scale same as above)
*   <b>PAY_6 </b>: Repayment status in April, 2005 (scale same as above)
*   <b>BILL_AMT1 </b>: Amount of bill statement in September, 2005 (NT dollar)
*   <b>BILL_AMT2 </b>: Amount of bill statement in August, 2005 (NT dollar)
*   <b>BILL_AMT3 </b>: Amount of bill statement in July, 2005 (NT dollar)
*   <b>BILL_AMT4 </b>: Amount of bill statement in June, 2005 (NT dollar)
*   <b>BILL_AMT5 </b>: Amount of bill statement in May, 2005 (NT dollar)
*   <b>BILL_AMT6 </b>: Amount of bill statement in April, 2005 (NT dollar)
*   <b>PAY_AMT1 </b>: Amount of previous payment in September, 2005 (NT dollar)
*   <b>PAY_AMT2 </b>: Amount of previous payment in August, 2005 (NT dollar)
*   <b>PAY_AMT3 </b>: Amount of previous payment in July, 2005 (NT dollar)
*   <b>PAY_AMT4 </b>: Amount of previous payment in June, 2005 (NT dollar)
*   <b>PAY_AMT5 </b>: Amount of previous payment in May, 2005 (NT dollar)
*   <b>PAY_AMT6 </b>: Amount of previous payment in April, 2005 (NT dollar)
*   <b>default_payment_next_month</b> : Target Variable: Default payment (1=yes, 0=no)

#Stage 1 - EDA,Visualization & Insights

## Descriptive Statistic

### Memeriksa missing value
"""

df.info()

"""Semua fitur dan label bertype integer"""

row_n,col_n=df.shape
missing_value = pd.DataFrame(df.isna().sum(), columns = ['Missing_Values'])
missing_value['Filling_Values (%)'] = (row_n-missing_value['Missing_Values'])*100/row_n
missing_value['Dtypes'] = df.dtypes.values
missing_value

"""Dataset tidak terdapat memiliki missing value.

### Memeriksa duplicated value
"""

df.duplicated().any()

"""## Melihat isi data"""

df.head()

df.tail()

df.sample(5)

df["ID"].nunique()

"""## Catatan
* Data ini berisi data 21000 nasabah bank dengan total 21000 baris dan 25 kolom
* Dataset memiliki data categorical dengan tipe data int
* Untuk memudahkan visualisasi , data categorical  akan dirubah kebentuk yang semestinya
* Untuk menyeragamkan, kolom PAY_0 akan diubah menjadi PAY_1
* Variabel target adalah kolom default_payment_next_month dengan tipe data int sementara kolom-kolom lainnya adalah variabel fitur

### Mengubah Tipe Data untuk Data Kategorikal
"""

# Mengubah tipe data untuk kolom SEX
df['SEX'] = df['SEX'].replace(1, "Male")
df['SEX'] = df['SEX'].replace(2, "Female")

# Menghitung banyak data untuk masing-masing kategori di kolom SEX
df_value_sex = df['SEX'].value_counts().reset_index()
df_value_sex.columns = ["SEX","COUNT"]
df_value_sex

# Menghitung banyak data untuk masing-masing kategori di kolom EDUCATION (sebelum tipe data diubah)
df['EDUCATION'].value_counts()

# Mengubah tipe data untuk kolom EDUCATION
df['EDUCATION'] = df['EDUCATION'].replace(1, "Graduate_School")
df['EDUCATION'] = df['EDUCATION'].replace(2, "University")
df['EDUCATION'] = df['EDUCATION'].replace(3, "High_School")
df['EDUCATION'] = df['EDUCATION'].replace(4, "Others")
df['EDUCATION'] = df['EDUCATION'].replace(5, "Others")
df['EDUCATION'] = df['EDUCATION'].replace(6, "Others")
df['EDUCATION'] = df['EDUCATION'].replace(0, "Others")

# Menghitung banyak data untuk masing-masing kategori di kolom EDUCATION (sesudah tipe data diubah)
df_value_education = df['EDUCATION'].value_counts().reset_index()
df_value_education.columns = ["EDUCATION","COUNT"]
df_value_education

# Menghitung banyak data untuk masing-masing kategori di kolom MARRIAGE (sebelum tipe data diubah)
df['MARRIAGE'].value_counts()

# Mengubah tipe data untuk kolom MARRIAGE
df['MARRIAGE'] = df['MARRIAGE'].replace(1, "Married")
df['MARRIAGE'] = df['MARRIAGE'].replace(2, "Single")
df['MARRIAGE'] = df['MARRIAGE'].replace(3, "Divorced")
df['MARRIAGE'] = df['MARRIAGE'].replace(0, "Unknown")

# Menghitung banyak data untuk masing-masing kategori di kolom MARRIAGE (sesudah tipe data diubah)
df_value_marriage = df['MARRIAGE'].value_counts().reset_index()
df_value_marriage.columns = ["MARRIAGE","COUNT"]
df_value_marriage

df.info()

"""### Pengelompokan Data"""

# Data kategorikal
category =['SEX','EDUCATION','MARRIAGE']
pay_delay = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']

# Data numerikal
nums = ['LIMIT_BAL', 'AGE',
        'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',
        'PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']

nums1 = ['LIMIT_BAL','AGE']

bill_amt = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']
pay_amt = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']



"""## Statistical Summary

### Data Kategorikal
"""

df[category].describe().T

"""Hasil Pengamatan Data Kategorikal :<br>
* Mayoritas data adalah female dengan frekuensi 12759
* Mayoritas education adalah university dengan frekuensi 9789
* Mayoritas status pernikahan adalah single dengan frekuensi 11184

### Data Numerikal
"""

df[nums].describe()

"""Hasil pengamatan :
* Mayoritas fitur memiliki nilai mean > median yang menandakan fitur cenderung berdistribusi positively skewed, hanya fitur AGE yang mendekati distribusi normal
* Data-data pada kolom numerikal kemungkinan memiliki outlier karena perbedaan antara nilai median dan nilai max pada datanya relatif jauh
* Terdapat nilai minus pada fitur BILL_AMT dan nilai 0 pada PAY_AMT

### Default Payment Next Month
"""

df["default_payment_next_month"].describe()

# Menghitung banyak nasabah yang default dan tidak default (0-tidak default, 1-default)
df_default_count = df["default_payment_next_month"].value_counts().reset_index()
df_default_count.columns = ["default_payment_next_month","count"]

# Mencari default rate
df_default_count["default_rate"] = df_default_count["count"] / 21000 * 100

df_default_count

plt.figure(figsize = (7,7))
y = df_default_count['default_rate']
mylabels = ['No Default','Default']
plt.pie(y, labels = mylabels,autopct='%1.0f%%',explode=[0,0.1])
plt.title('Percentage Nasabah Yang Default',bbox={'facecolor':'0.8', 'pad':5})
plt.show()

"""Hasil pengamatan :
* Jumlah nasabah yang mengalami default lebih sedikit daripada jumlah nasabah yang tidak mengalami default
* Meskipun jumlah nasabah yang mengalami default lebih sedikit, default rate nasabah di bank tersebut relatif besar yaitu sebesar 22% (umumnya default rate nasabah di sebuah bank di Taiwan adalah 2%)

### Rangkuman

1. Dataset memiliki 21000 baris dan 25 kolom
2. Semua kolom tertype integer
3. Fitur PAY_0 tidak sesuai urut akan diganti PAY_1
4. Untuk memudahkan visualisasi , data kategorikal akan dirubah kebentuk yang semestinya
5. Variabel target adalah kolom default_payment_next_month dengan tipe data int sementara kolom-kolom lainnya adalah variabel fitur
6. Tidak terdapat duplikat data dan missing value
7. Mayoritas nasabah adalah perempuan,dengan pendidikan University dan berstatus Single
8. Nilai unique pada fitur kategorikal tidak banyak
9. Jumlah nasabah yang default adalah  22%
10. Pada BILL_AMT3 memiliki nilai max 1.664089e+06 dan PAY_AMT3 nilai max 1.684259e+06 nilai ini terlalu tinggi dalam tagihan dan pembayaran dimana kredit limit maximal 800000, diduga data typo atau salah input akan dilakukan handling outlier

## Univariate Analysis

### Memeriksa Outlier Menggunakan Boxplot

### ID
"""

# Membuat boxplot untuk data pada kolom ID
plt.figure(figsize=(5, 5))
sns.boxplot(y=df["ID"], color='blue', orient='v')
plt.show()

"""### AGE dan LIMIT_BALL"""

# Membuat boxplot untuk data pada kolom LIMIT_BAL, dan AGE
plt.figure(figsize=(10, 5))
for i in range(0, len(nums1)):
    plt.subplot(1, len(nums1), i+1)
    sns.boxplot(y=df[nums1[i]], color='blue', orient='v')
    plt.tight_layout()

"""### PAY_AMT"""

# Membuat boxplot untuk data pada kolom PAY_AMT1 sampai PAY_AMT6
plt.figure(figsize=(12, 5))
for i in range(0, len(pay_amt)):
    plt.subplot(1, len(pay_amt), i+1)
    sns.boxplot(y=df[pay_amt[i]], color='blue', orient='v')
    plt.tight_layout()

"""### BILL_AMT"""

# Membuat boxplot untuk data pada kolom BILL_AMT1 sampai BILL_AMT6
plt.figure(figsize=(12, 5))
for i in range(0, len(bill_amt)):
    plt.subplot(1, len(bill_amt), i+1)
    sns.boxplot(y=df[bill_amt[i]], color='blue', orient='v')
    plt.tight_layout()

"""## Memeriksa Distribusi Data

### AGE dan LIMIT_BALL
"""

# Memeriksa distribusi data untuk data pada kolom LIMIT_BAL dan AGE
plt.figure(figsize=(12, 5))
for i in range(0, len(nums1)):
    plt.subplot(2, len(nums1), i+1)
    sns.distplot(df[nums1[i]], color='gray')
    plt.tight_layout()

df["LIMIT_BAL"].value_counts().reset_index().head(5)

"""Limit dengan nasabah terbanyak : NTD 20.000, NTD 30.000, <b>NTD 50.000</b>, NTD 80.000, NTD 200.000"""

df["AGE"].value_counts().reset_index().head(5)

"""Nasabah bank ini paling banyak berusia 26 tahun, 27 tahun, 28 tahun, **29 tahun**, 30 tahun.

### PAY_AMT
"""

# Memeriksa distribusi data untuk data pada kolom PAY_AMT1 sampai PAY_AMT6
plt.figure(figsize=(15, 12))
for i in range(0, len(pay_amt)):
    plt.subplot(len(pay_amt),2, i+1)
    sns.distplot(df[pay_amt[i]], color='gray')
    plt.tight_layout()

"""### BILL_AMT"""

# Memeriksa distribusi data untuk data pada kolom BILL_AMT1 sampai BILL_AMT6
plt.figure(figsize=(15, 12))
for i in range(0, len(bill_amt)):
    plt.subplot(len(bill_amt),2, i+1)
    sns.distplot(df[bill_amt[i]], color='gray')
    plt.tight_layout()

"""### PAY"""

# Memeriksa distribusi data untuk data pada kolom PAY_1 sampai PAY_6
plt.figure(figsize=(15, 12))
for i in range(0, len(pay_delay)):
    plt.subplot(len(pay_delay),2, i+1)
    sns.distplot(df[pay_delay[i]], color='gray')
    plt.tight_layout()

"""### SEX"""

# Melihat persebaran data pada kolom SEX
df_value_sex.plot(x = "SEX", kind = "bar")
plt.xticks(rotation = 0)
plt.show()

"""### EDUCATION"""

# Melihat persebaran data pada kolom EDUCATION
plt.figure(figsize=(10,10))
df_value_education.plot(x = "EDUCATION", kind = "bar")
plt.xticks(rotation = 0)
plt.show()

"""### MARRIAGE"""

# Melihat persebaran data pada kolom MARRIAGE
df_value_marriage.plot(x = "MARRIAGE", kind = "bar")
plt.xticks(rotation = 0)
plt.show()

"""Hasil Pengamatan : <br>
1. Mayoritas fitur memiliki outlier,kecuali ID,AGE dan LIMIT_BALL
2. BILL_AMT dan PAY_AMT distribusi positively skewed
3. Terdapat nilai minus/negatif pada fitur BILL_AMT

## Multivariate Analysis

### Memeriksa Korelasi
"""

plt.figure(figsize=(15, 15))
sns.heatmap(df.corr(), cmap='Oranges', annot=True, fmt='.2f')
plt.show()

"""Hasil pengamatan :
* Korelasi paling kuat terjadi antara kolom PAY_X dengan kolom PAY_X lainnya (X = 1,2,3,4,5,6) dan antara kolom BILL_AMTX dengan kolom BILL_AMTX lainnya (X = 1,2,3,4,5,6).
* Dari nilai korelasi antara BILL_AMT6 dengan BILL_AMT5, BILL_AMT5 dengan BILL_AMT4, BILL_AMT3 dengan BILL_AMT2, dan BILL_AMT2 dengan BILL_AMT1 adalah akumulasi dari jumlah tagihan atau BILL_AMT yang sudah dibayar atau belum dibayar.
  * Hal ini dapat dipengaruhi oleh adanya bunga kartu kredit dan biaya keterlambatan pembayaran.
* Nilai korelasi antara PAY_6 dengan PAY_5 adalah 0.82, PAY_5 dengan PAY_4 berkorelasi sebesar 0.83, PAY_4 dengan PAY_3 berkorelasi sebesar 0.78, PAY_3 dengan PAY_2 berkorelasi sebesar 0.77, PAY_2 dengan PAY_1 berkorelasi sebesar 0.67.
  * Dari sini dapat dilihat bahwa hubungan antara repayment status seorang nasabah dari bulan satu ke bulan lainnya semakin lama cenderung semakin lemah (dari PAY_6 ke PAY_1).
  * Dari nilai-nilai korelasi ini didapat juga bahwa apabila di bulan sebelumnya nasabah menunggak pembayaran tagihan kartu kredit, maka di bulan berikutnya nasabah tersebut juga kemungkinan besar akan kembali menunggak. Begitu pula sebaliknya. Jika di bulan sebelumnya nasabah melakukan pembayaran tepat waktu / tidak menunggak, maka kemungkinan besar di bulan berikutnya nasabah juga tidak akan menunggak.
* Di antara kolom-kolom lainnya, kolom yang paling berkorelasi dengan variabel target adalah kolom PAY_1 yang kemudian diikuti dengan kolom PAY_2, PAY_3, PAY_4, PAY_5, dan PAY_6.
* Limit nasabah memiliki korelasi positif yang lemah (sampai dengan 0.3) dengan jumlah tagihan nasabah (BILL_AMT).
  * Ada kemungkinan semakin besar limit nasabah, semakin besar juga jumlah tagihannya. Begitu pula sebaliknya.
* Limit nasabah memiliki korelasi negatif yang lemah dengan repayment status (PAY_1 sampai PAY_6).
  * Ada kemungkinan semakin besar limit nasabah, semakin kecil nilai pada status repaymentnya. Begitu pula sebaliknya.

## Pairplot

### PAY
"""

plt.figure(figsize=(15, 15))
sns.pairplot(df[pay_delay], diag_kind='kde')

"""### BILL_AMT"""

plt.figure(figsize=(15, 15))
sns.pairplot(df[bill_amt], diag_kind='kde')

"""## Bivariate Analysis

### Kategori Umur dan Limit
"""

# kategori umur
def get_age_cat(age):
    if age > 60:
        cat = "Senior Citizen"
    elif age >= 46:
        cat = "Senior Adults"
    elif age >= 36:
        cat = "Middle-Aged Adults"
    elif age >= 20:
        cat = "Young Adults"
    return cat

df["LIMIT_BAL"].describe()

# kategori limit
def get_limit_cat(limit):
    if limit > 240000:
        cat = "High"
    elif limit > 140000:
        cat = "Medium"
    else:
        cat = "Low"
    return cat

df1 = df

df1["age_cat"] = df1["AGE"].apply(lambda x : get_age_cat(x))
df1["limit_cat"] = df1["LIMIT_BAL"].apply(lambda x : get_limit_cat(x))

df1.sample(5)

"""### Default Percentage untuk Masing-masing Kategori Usia

Catatan : <br>
* Young Adults 	(20-35 tahun)
* Middle-Aged Adults 	 (36-45 tahun)
* Senior Adults 	 (46-59 tahun)
* Senior Citizen (60 tahun ke atas)
"""

df_age = df1.groupby(["age_cat","default_payment_next_month"])["ID"].count().reset_index()

df_age = pd.pivot_table(df_age, index = "age_cat", columns = "default_payment_next_month", values = "ID")
df_age = df_age.reset_index()
df_age.columns = ['age_cat','NON DEFAULT','DEFAULT']

df_age["total"] = df_age["NON DEFAULT"] + df_age["DEFAULT"]
df_age["default"] = round(df_age["DEFAULT"] / df_age["total"] * 100,2)          # dp_per_agecat
df_age["nondefault"] = round(df_age["NON DEFAULT"] / df_age["total"] * 100,2)   # ndp_per_agecat
df_age["dp_per_total_default"] = round(df_age["DEFAULT"] / 4645 * 100,2)
df_age["dp_per_all"] = df_age["DEFAULT"] / 21000 * 100


df_age = df_age.sort_values("total", ascending = False)
df_age

df_age.plot(x = "age_cat", y = "default", kind = "bar",figsize = (8,5), ylim = (0,28),color = ["#afafaf","#afafaf","#1f77b4","#1f77b4"],legend = None)
plt.xticks(rotation = 0, fontsize = 12)
plt.xlabel("AGE", fontsize = 12)
plt.title("Default Probability for Age Category", fontsize = 15)
# plt.axhline(y=25.1, color='red', ls='--', lw=1.5)
plt.text(1.8, 25.5, s='24.65%',fontsize=11,color="#1f77b4",weight="bold")
plt.text(2.81, 24.8, s='23.94%',fontsize=11,color="#1f77b4",weight="bold")
plt.show()

"""Berdasarkan kategori usia, mayoritas nasabah yang default adalah Senior Adult dan Senior Citizen

### Melihat Default Percentage untuk Semua Usia
"""

df_age1 = df1[(df1["default_payment_next_month"] == 1)]["AGE"].value_counts().reset_index()
df_age1.columns = ["AGE","DEFAULT"]

df_age1["dp_per_total_default"] = df_age1["DEFAULT"] / 4645 * 100
df_age1["dp_per_all"] = df_age1["DEFAULT"] / 21000 * 100

df_age1.sort_values("AGE",ascending = True, inplace = True)

df_age1

plt.figure(figsize=(15, 15))
df_age1.plot(x = "AGE", y = "DEFAULT", kind = "line", grid = True)
plt.xticks(range(21, 75, 3))
plt.title("Banyak Nasabah yang Default untuk Semua Usia")
plt.axvline(x=21, color='blue', ls='--', lw=1.5)
plt.axvline(x=27, color='blue', ls='--', lw=1.5)
plt.axvline(x=29, color='blue', ls='--', lw=1.5)
plt.legend(loc = 1)
plt.show()

"""Hasil pengamatan :
* Nasabah yang paling banyak default memiliki usia 25, 27, dan 29 tahun.

### Default Percentage untuk Masing-masing Kategori Limit
"""

df_limit = df.groupby(["limit_cat","default_payment_next_month"])["ID"].count().reset_index()

df_limit = pd.pivot_table(df_limit, index = "limit_cat", columns = "default_payment_next_month", values = "ID")
df_limit = df_limit.reset_index()
df_limit.columns = ["limit_cat","NON DEFAULT","DEFAULT"]

df_limit["total"] = df_limit["NON DEFAULT"] + df_limit["DEFAULT"]
df_limit["default"] = round(df_limit["DEFAULT"] / df_limit["total"] * 100,2)            #dp_per_cat
df_limit["nondefault"] = round(df_limit["NON DEFAULT"] / df_limit["total"] * 100,2)     #ndp_per_cat
df_limit["dp_per_total_default"] = round(df_limit["DEFAULT"] / 4645 * 100,2)
df_limit["dp_per_all"] = df_limit["DEFAULT"] / 21000 * 100

df_limit = df_limit.sort_values("total", ascending = False)
df_limit

df_limit.plot(x = "limit_cat", y = "default", kind = "bar",figsize = (6,5), color = ["#1f77b4","#afafaf","#afafaf"], legend = None, ylim = (0,32))
plt.xticks(rotation = 0, fontsize = 12)
plt.xlabel("LIMIT BALANCE", fontsize = 12)
plt.text(-0.185, 29.5, s='28.51%',fontsize=11,color="#1f77b4",weight="bold")
plt.title("Default Probability for Limit Category", fontsize = 15)
plt.show()

"""Berdasarkan kategori limit, mayoritas nasabah yang default memiliki limit kartu kredit di bawah <b>NTD 200000</b> (kategori low).

### Default Percentage untuk Masing-masing Gender
"""

df_sex = df1.groupby(["SEX","default_payment_next_month"])["ID"].count().reset_index()

df_sex = pd.pivot_table(df_sex, index = ["SEX"], columns = ["default_payment_next_month"], values = ["ID"])
df_sex = df_sex.reset_index()
df_sex.columns = ['SEX','NON DEFAULT','DEFAULT']

df_sex["total"] = df_sex["NON DEFAULT"] + df_sex["DEFAULT"]
df_sex["default"] = round(df_sex["DEFAULT"] / df_sex["total"] * 100,2)        # default percentage per sex
df_sex["nondefault"] = round(df_sex["NON DEFAULT"] / df_sex["total"] * 100,2) # nondefault percentage per sex
df_sex["dp_per_total_default"] = df_sex["DEFAULT"] / 4645 * 100
df_sex["dp_per_all"] = df_sex["DEFAULT"] / 21000 * 100

df_sex

df_sex.plot(x = "SEX", y = "dp_per_total_default",kind = "bar",figsize = (6,5), color = ["#d51961","#1f77b4"],legend = None)
plt.xlabel("SEX",fontsize = 12)
plt.xticks(rotation = 0, fontsize = 12)
plt.title("Default Probability for Sex Category", fontsize = 15)
plt.text(-0.13, 25, s='57,31%',fontsize=12,color="black",weight="bold")
plt.show()

df_sex.plot(x = "SEX", y = "default",kind = "bar",figsize = (6,5), ylim = (0,28), color = ["#d51961","#1f77b4"],legend = None)
plt.xticks([0,1],["Female","Male"],rotation = 0, fontsize = 12)
plt.xlabel("SEX",fontsize = 12)
plt.title("Default Probability for Sex Category Per Subcategory", fontsize = 15)
plt.text(0.9, 25, s='24.06%',fontsize=12,color="#1f77b4",weight="bold")
plt.show()

"""Berdasarkan jenis kelamin, mayoritas nasabah yang default adalah perempuan yaitu sebanyak  57,31% (2662 orang) dari total nasabah yang default.

Berdasarkan rasio per subcategory, 24,06% dari nasabah laki-laki mengalami default.

### Default Percentage untuk Masing-masing Kategori Pendidikan
"""

df1['EDUCATION'] = df1['EDUCATION'].replace(1, "Graduate School")
df1['EDUCATION'] = df1['EDUCATION'].replace(2, "University")
df1['EDUCATION'] = df1['EDUCATION'].replace(3, "High School")
df1['EDUCATION'] = df1['EDUCATION'].replace(4, "Others")
df1['EDUCATION'] = df1['EDUCATION'].replace(5, "Others")
df1['EDUCATION'] = df1['EDUCATION'].replace(6, "Others")
df1['EDUCATION'] = df1['EDUCATION'].replace(0, "Others")

df_edu = df1.groupby(["EDUCATION","default_payment_next_month"])["ID"].count().reset_index()

df_edu = pd.pivot_table(df_edu, index = ["EDUCATION"], columns = ["default_payment_next_month"], values = ["ID"])
df_edu = df_edu.reset_index()
df_edu.columns = ['EDUCATION','NON DEFAULT','DEFAULT']

df_edu["total"] = df_edu["NON DEFAULT"] + df_edu["DEFAULT"]
df_edu["default"] = round(df_edu["DEFAULT"] / df_edu["total"] * 100,2)  # dp_per_education
df_edu["nondefault"] = round(df_edu["NON DEFAULT"] / df_edu["total"] * 100,2)  # ndp_per_education
df_edu["dp_per_total_default"] = round(df_edu["DEFAULT"] / 4645 * 100,2)
df_edu["dp_per_all"] = df_edu["DEFAULT"] / 21000 * 100

df_edu = df_edu.sort_values("default", ascending = False)
df_edu

df_edu.plot(x = "EDUCATION", y = "default", kind = "bar",figsize = (7,5), color = ["#1474ca","#428fd4","#89b9e4","#a1c7e9"], legend = None, ylim = (0,28))
plt.xticks(rotation = 0, fontsize = 12)
plt.xlabel("EDUCATION", fontsize = 12)
# plt.axhline(y=24.49, color='red', ls='--', lw=1.5)
plt.text(-0.22, 25.5, s='24.49%',fontsize=11,color="#1f77b4",weight = "bold")
plt.text(0.77, 24.8, s='23.96%',fontsize=11,color="#1f77b4",weight = "bold")
plt.title("Default Probability for Education Category", fontsize = 15)
plt.show()

"""Berdasarkan kategori pendidikan, mayoritas nasabah yang default adalah lulusan High School dan  universitas.

## Default Percentage untuk Masing-masing Status Pernikahan
"""

df['MARRIAGE'] = df['MARRIAGE'].replace(1, "Married")
df['MARRIAGE'] = df['MARRIAGE'].replace(2, "Single")
df['MARRIAGE'] = df['MARRIAGE'].replace(3, "Divorced")
df['MARRIAGE'] = df['MARRIAGE'].replace(0, "Unknown")

df_marriage = df.groupby(["MARRIAGE","default_payment_next_month"])["ID"].count().reset_index()

df_marriage = pd.pivot_table(df_marriage, index = ["MARRIAGE"], columns = ["default_payment_next_month"], values = ["ID"])
df_marriage = df_marriage.reset_index()
df_marriage.columns = ['MARRIAGE','NON DEFAULT','DEFAULT']

df_marriage["total"] = df_marriage["NON DEFAULT"] + df_marriage["DEFAULT"]
df_marriage["default"] = round(df_marriage["DEFAULT"] / df_marriage["total"] * 100,2)         # dp_per_marriage
df_marriage["nondefault"] = round(df_marriage["NON DEFAULT"] / df_marriage["total"] * 100,2)  # ndp_per_marriage
df_marriage["dp_per_total_default"] = round(df_marriage["DEFAULT"] / 4645 * 100,2)
df_marriage["dp_per_all"] = df_marriage["DEFAULT"] / 21000 * 100

df_marriage

df_marriage.plot(x = "MARRIAGE", y = "default", kind = "bar",figsize = (7,5), legend = None, ylim = (0,32),color = ["#DA70D6","#F3889E","#00bbd4","#003d37"])
plt.xticks(rotation = 0, fontsize = 12)
plt.xlabel("MARRIAGE STATUS", fontsize = 12)
plt.text(-0.23, 29.8, s='28.77%',fontsize=11,color="#1f77b4", weight="bold")
plt.title("Default Probability for Marriage Status Category", fontsize = 15)
plt.show()

"""Berdasarkan status pernikahan, diposisi pertama ada Divorced dengan persentase 28.77% dan disusul oleh Married dan Single

### Hubungan Status Pernikahan dan Limit Kartu Kredit
"""

df_marriage1 = round(df1.groupby("MARRIAGE").agg({"LIMIT_BAL" : ["mean","median"]}).reset_index(),2)
df_marriage1.columns = ["marriage", "mean_limit", "median_limit"]
df_marriage1

df_marriage1.plot(x = "marriage", kind = "bar")
plt.xticks([0,1,2,3],["unknown","married","single","divorced"],rotation = 0)
plt.title("Status Pernikahan dan Limit Credit Card")
plt.show()

"""Nasabah yang memiliki limit kartu kredit paling tinggi adalah nasabah yang sudah menikah. Hal ini masuk akal mengingat bertambahnya kebutuhan hidup setelah menikah.

### Hubungan Pendidikan dan Limit Kartu Kredit
"""

# Menghitung mean dan median limit untuk masing-masing kategori pendidikan
df_edu1 = round(df1.groupby("EDUCATION").agg({"LIMIT_BAL" : ["mean","median"]}).reset_index(),2)
df_edu1.columns = ["EDUCATION", "mean_limit", "median_limit"]

df_edu1['EDUCATION'] = df_edu1['EDUCATION'].replace("Graduate_School", 1)
df_edu1['EDUCATION'] = df_edu1['EDUCATION'].replace("University", 2)
df_edu1['EDUCATION'] = df_edu1['EDUCATION'].replace("High_School", 3)
df_edu1['EDUCATION'] = df_edu1['EDUCATION'].replace("Others", 4)

df_edu1 = df_edu1.sort_values("EDUCATION",ascending = True)
df_edu1 = df_edu1.reset_index()
df_edu1.drop(["index"],axis = 1)

df_edu1.plot(x = "EDUCATION", y = ["mean_limit","median_limit"],kind = "line")
plt.xticks([1,2,3,4],["graduate school","university","high school","others"],rotation = 0)
plt.legend(loc = 1)
plt.axvline(x=1, color='gray', ls='--', lw=1.5)
plt.axvline(x=3, color='gray', ls='--', lw=1.5)
plt.title("Pendidikan Terakhir dan Limit Credit Card")
plt.show()

"""Semakin rendah tingkat pendidikan terakhir nasabah bank, semakin rendah juga limit kartu kredit nasabah tersebut.

### Hubungan Usia dan Limit Kartu Kredit
"""

df_age1 = round(df1.groupby("age_cat").agg({"LIMIT_BAL" : ["mean","median"]}).reset_index(),1)
df_age1.columns = ["age_cat", "mean_limit", "median_limit"]
df_age1

df_age1.plot(x = "age_cat", kind = "bar")
plt.xticks(rotation = 0)
plt.title("Kelompok Usia dan Limit Credit Card")
plt.show()

"""Nasabah dengan kategori umur  Senior Citizen  memiliki limit paling tinggi diantara nasabah yang lainnya. Urutan kedua adalah Middle-Aged Adults. Sementara urutan ketiga adalah Senoir Adylts dan Young Adults.

### Hubungan Jenis Kelamin dan Limit Kartu Kredit
"""

df_sex1 = round(df1.groupby("SEX").agg({"LIMIT_BAL" : ["mean","median"]}).reset_index(),1)
df_sex1.columns = ["SEX", "mean_limit", "median_limit"]
df_sex1

df_sex1.plot(x = "SEX", kind = "bar")
plt.xticks([0,1],["male","female"],rotation = 0)
plt.title("Gender dan Limit Credit Card")
plt.show()

"""Dilihat dari nilai rata-rata limit credit card male lebih tinggi dibandingkan female

# Business Insight
1. Dari 21000 nasabah, sebanyak 4645 nasabah mengalami default payment pada tagihan kartu kreditnya. Meskipun nasabah yang mengalami default ini lebih sedikit jumlahnya daripada yang tidak mengalami default, default rate yang diperoleh yaitu sebesar 22% melebihi batas maksimum default rate bank secara umum di Taiwan.

2. Dari total 4645 nasabah yang default, 57,31% adalah nasabah wanita. Lebih lanjut, dilihat dari mean dan mediannya, nasabah wanita memiliki limit yang lebih tinggi daripada nasabah laki-laki.

3. Nasabah yang cenderung memiliki potensi default yang tinggi berusia di rentang 25 tahun sampai 30 tahun.

4. Berdasarkan status pernikahannya, 50.85% dari nasabah yang default memiliki status single dan di posisi kedua, 47.75% dari nasabah yang default memiliki status married.

5. Semakin besar limit kartu kredit nasabah, semakin sedikit nasabah yang mengalami default. 75% dari nasabah yang default memiliki limit kartu kredit yang rendah yaitu kurang dari NTD 200000.

6. Semakin rendah tingkat pendidikan terakhir nasabah , semakin rendah juga limit credit nasabah tersebut.

##Rekomendasi Bisnis <br>
1. Memberikan kemudahan untuk mengajukan peningkatan limit kepada pemegang kartu kredit yang melakukan payment tepat waktu.

2. Nasabah yang terdeteksi berpotensi akan gagal bayar segera dihubungi lebih dulu dan ditawarkan solusi.

## Rekomendasi Pre-Processing

1. Kolom ID akan di drop karena merupakan identifier dari tiap baris yang nilainya unik dan tidak dapat memberikan informasi apa-apa dalam analisis.
2. Merubah kolom PAY_0 menjadi PAY_1
3. Terdapat nilai yang belum terdefinisi pada kolom EDUCATION akan diubah menjadi unknown
4. Terdapat nilai minus pada kolom BILL_AMT1 - BILL_AMT6 hal itu wajar terjadi tetapi nilai tersebut akan di drop karena outlier
5. Pada visualisasi boxplot terdapat banyak outlier di kolom PAY_AMT1 - PAY_AMT 6 dan BILL_AMT1-BILL_AMT6, untuk penanganannya akan memfilter outlier dengan menggunakan z-score , kemudian akan dicek distribusi datanya setelah difilter
6. Melakukan sedikit experiment perbandingan antara z-score dengan IQR untuk pengematan perbandingan hasil akurasinya
7. Kolom PAY, BILL_AMT,PAY_AMT memiliki hubungan sebab akibat,kita akan memilih salah satu nya atau akan kami pertimbangkan lebih lanjut untuk tidak drop kolom tersebut sebagai bahan pertimbangan akurasi model kedepannya

# Stage 2 - Data Pre-Processing
"""

df = pd.read_csv('/content/gdrive/MyDrive/dataset/train_20D8GL3.csv')

"""### Mengubah nama kolom PAY_0 menjadi PAY_1"""

df.rename(columns = {'PAY_0': 'PAY_1'},inplace = True)

df.info()

"""## Encoding"""

# Mengubah tipe data untuk kolom MARRIAGE
df['MARRIAGE'] = df['MARRIAGE'].replace(1, "Married")
df['MARRIAGE'] = df['MARRIAGE'].replace(2, "Single")
df['MARRIAGE'] = df['MARRIAGE'].replace(3, "Divorced")
df['MARRIAGE'] = df['MARRIAGE'].replace(0, "Unknown")

OHC = pd.get_dummies(df['MARRIAGE'], prefix = 'status')
df = df.join(OHC)

for i in ['PAY_1', 'PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'] :
    OHC = pd.get_dummies(df[i], prefix = i)
    df = df.join(OHC)

df.head(2)

"""##Split Data Train & Test"""

from sklearn.model_selection import train_test_split
train,test = train_test_split(df, test_size=0.3, random_state=42)

train.info()

test.info()

# Data kategorikal
category =['SEX','EDUCATION','MARRIAGE']
pay_delay = ['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']

# Data numerikal
nums = ['LIMIT_BAL', 'AGE',
        'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',
        'PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']

lim_age = ['LIMIT_BAL','AGE']

bill_amt = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']
pay_amt = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']

"""## Handling Outlier"""

#sebelum handling outlier
plt.figure(figsize=(20,25))
for i in range(len(nums)):
    plt.subplot(len(nums)//2, 7,i+1)
    ax =sns.boxplot(
        y = train[nums[i]]
    )
    ax.set_xlabel(str(nums[i]), fontsize = 20)
    ax.tick_params(labelsize = 20)
plt.tight_layout()
plt.show()

"""### Z-Score"""

from scipy import stats
# Menghapus outlier berdasarkan Z-score
filter = np.array([True] * len(train))

for x in nums:
    zscore = abs(stats.zscore(train[x]))
    filter = (zscore < 3) & filter

train_zscore = train[filter]
print('Jumlah baris sebelum outliers :',len(train))
print('Jumlah baris sesudah outliers :',len(train_zscore))
print('percent of outlier: ', train_zscore.shape[0]/train.shape[0] * 100, '%')

print('=========================================================== Visualisasi Setelah Menggunakan ZSCORE ===========================================================')

plt.figure(figsize=(20, 20))
for i in range(len(nums)):
    plt.subplot(len(nums)//2, 7, i+1)
    ax = sns.boxplot(
        y = train_zscore[nums[i]]
        )
    ax.set_xlabel(str(nums[i]), fontsize = 20)
    ax.tick_params(labelsize=16)
plt.tight_layout()
plt.show()

"""## Transformation"""

from scipy.sparse import issparse
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.preprocessing import RobustScaler

"""## Distribusi Data Sebelum Feature Transformation

###BILL_AMT
"""

#BILL_AMT
a = plt.figure(figsize=(20,15))

for i, j in enumerate(bill_amt):
    a.add_subplot(2, 3, i+1)
    sns.distplot(train_zscore[j])
    plt.legend()
    a.tight_layout()

"""### PAY_AMT"""

#PAY_AMT
a = plt.figure(figsize=(20,15))

for i, j in enumerate(pay_amt):
    a.add_subplot(2, 3, i+1)
    sns.distplot(train_zscore[j])
    plt.legend()
    a.tight_layout()

"""###LIMIT_BALL & AGE"""

a = plt.figure(figsize=(20,15))

for i, j in enumerate(nums1):
    a.add_subplot(2, 3, i+1)
    sns.distplot(train_zscore[j])
    plt.legend()
    a.tight_layout()

"""## Skewness

### Sebelum Handling Outlier
"""

print(df[bill_amt].skew())
print()
print(df[pay_amt].skew())
print()

"""### Sesudah Handling Outlier"""

print(train_zscore[bill_amt].skew())
print()
print(train_zscore[pay_amt].skew())
print()

"""## Proses Scaling

###Scaling Data Train
"""

#Scaling Train LIMIT_BILL dan AGE
scaler = StandardScaler()
train_zscore[lim_age] = scaler.fit_transform(train_zscore[lim_age])
train_zscore[lim_age].describe()

#Scaling BILL_AMT
train_zscore[bill_amt] = scaler.fit_transform(train_zscore[bill_amt])
train_zscore[bill_amt].describe()

#Scaling PAY_AMT
train_zscore[pay_amt] = scaler.fit_transform(train_zscore[pay_amt])
train_zscore[pay_amt].describe()

train_zscore.info()

train = train_zscore

"""### Scaling Data Test"""

#Scaling Test Limit Bill dan Age
test[lim_age] = scaler.fit_transform(test[lim_age])
test[lim_age].describe()

#Scaling BILL_AMT
test[bill_amt] = scaler.fit_transform(test[bill_amt])
test[bill_amt].describe()

#Scaling PAY_AMT
test[pay_amt] = scaler.fit_transform(test[pay_amt])
test[pay_amt].describe()

test.info()

train = train.drop(columns = ['ID','MARRIAGE','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])
test = test.drop(columns = ['ID','MARRIAGE','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])

train.info()

train.describe()

test.info()

test.describe()

"""### Kesimpulan

Kami sudah melakukan beberapa percobaan/eksperimen menggunakan yeojohnson,log, log(x+1),findlog pada fitur PAY_AMT1 - PAY_AMT6 namun yang kami dapatkan dari percobaan tersebut adalah :
1. Menggunakan log tranform membuat nilainya menjadi -inf
2. Menggunakan log(x+1),findlog dan yejohnson setelah divisualisasikan plot distribusinya menjadi punuk unta

Untuk kolom LIMIT_BAL,AGE, dan BILL_AMT1 - BILL_AMT6 setelah handling outlier distribusi datanya sudah mendekati normal jadi kami hanya perlu melakukan StandardScaler <br>
Kami memutuskan untuk menggunakan StandardScaler karena nilai negatif pada kolom BILL_AMT wajar terjadi

# Stage 3 - Machine Learning Modelling & Evaluation
"""

#Memisahkan x,y
x_train = train.drop(['default_payment_next_month'],axis=1)
y_train = train['default_payment_next_month']

x_test = test.drop(['default_payment_next_month'],axis=1)
y_test = test['default_payment_next_month']

print(x_train.shape)
print(y_train.shape)

print(x_test.shape)
print(y_test.shape)

y_train.value_counts(normalize=True)

y_test.value_counts(normalize=True)

"""##Logistic Regression"""

from sklearn.metrics import f1_score, roc_auc_score,recall_score,accuracy_score,confusion_matrix

def eval_classification(model):
    y_pred = model.predict(x_test)
    y_pred_train = model.predict(x_train)
    y_pred_proba = model.predict_proba(x_test)
    y_pred_proba_train = model.predict_proba(x_train)
    y_pred_datatest = model.predict(x_test)

    cm = confusion_matrix(y_test, y_pred_datatest)
    tn, fp, fn, tp = cm.ravel()

    print("Accuracy (Test Set): ", accuracy_score(y_test, y_pred))
    print("F1-Score (Test Set): " , f1_score(y_test, y_pred))
    print("Recall (Test Set): " , recall_score(y_test, y_pred))
    print("roc_auc (test-proba): " , roc_auc_score(y_test, y_pred_proba[:, 1]))
    print("roc_auc (train-proba): " , roc_auc_score(y_train, y_pred_proba_train[:, 1]))
    print('TP  : {}\nFP  : {}\nFN  : {}\nTN  : {}\n'.format(tp, fp, fn, tn))


def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=x.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

from sklearn.linear_model import LogisticRegression

def draw_learning_curve(param_values):
    train_scores = []
    test_scores = []

    for c in param_values:
        model = LogisticRegression(penalty='l2', C=c)
        model.fit(x_train, y_train)

        # eval on train
        y_pred_train_proba = model.predict_proba(x_train)
        train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
        train_scores.append(train_auc)

        # eval on test
        y_pred_proba = model.predict_proba(x_test)
        test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
        test_scores.append(test_auc)

        print('param value: ' + str(c) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

    plt.plot(param_values, train_scores, label='Train')
    plt.plot(param_values, test_scores, label='Test')
    plt.xlabel('C')
    plt.ylabel('AUC')
    plt.title('Learning Curve - Hyperparameter C - Logistic Regression')
    plt.legend()
    plt.show()

"""### FIT MODEL"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(x_train, y_train)
eval_classification(logreg)

"""### Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV

#dengan class_weight(jika data imbalance)
penalty = ['l1', 'l2']
C = [float(x) for x in np.linspace(0.0001, 1, 100)]
class_weight = 'balanced'
hyperparameters = dict(penalty=penalty, C=C,class_weight=class_weight)

logreg = LogisticRegression()
rs = RandomizedSearchCV(logreg, hyperparameters, scoring='roc_auc', random_state=42)
rs.fit(x_train, y_train)
eval_classification(rs)

# learning curve
param_values = [float(x) for x in np.linspace(0.0001, 1, 100)]
draw_learning_curve(param_values)

"""# kNN"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
def eval_classification(model):
    ypred_proba_test = model.predict_proba(x_test)
    ypred_proba_train = model.predict_proba(x_train)
    ypred_datatest = model.predict(x_test)
    ypred_datatrain = model.predict(x_train)

    cm = confusion_matrix(y_test, ypred_datatest)
    tn, fp, fn, tp = cm.ravel()

    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, ypred_datatest))
    print("Precision (Test Set): %.2f" % precision_score(y_test, ypred_datatest))
    print("Recall (Test Set): %.2f" % recall_score(y_test, ypred_datatest))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, ypred_datatest))
    print("AUC (test-proba): %.2f" % roc_auc_score(y_test, ypred_proba_test[:,1] ))
    print("AUC (train-proba): %.2f" % roc_auc_score(y_train, ypred_proba_train[:,1] ))
    print('TP  : {}\nFP  : {}\nFN  : {}\nTN  : {}\n'.format(tp, fp, fn, tn))

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
eval_classification(knn)

"""### Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV
n_neighbors = [int(x) for x in np.linspace(1, 100, 100)]
weights = ['uniform','distance']
p=[1,2]
algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']
hyperparameters = dict( n_neighbors = n_neighbors,
                        weights = weights,
                        p = p,
                        algorithm = algorithm
                       )
knn.fit(x_train, y_train)
knn_model = RandomizedSearchCV(knn, hyperparameters, cv=5, random_state=1, scoring='roc_auc')
knn_model.fit(x_train, y_train)
eval_classification(knn_model)

# Analyzing Learning Curve
import numpy as np
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

def draw_learning_curve(param_values):
    train_scores = []
    test_scores = []

    for i in param_values:
        model = KNeighborsClassifier(n_neighbors=i)
        model.fit(x_train, y_train)

        # eval on train
        y_pred_train_proba = model.predict_proba(x_train)
        train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
        train_scores.append(train_auc)

        # eval on test
        y_pred_proba = model.predict_proba(x_test)
        test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
        test_scores.append(test_auc)

        print('param value: ' + str(i) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

    plt.plot(param_values, train_scores, label='Train')
    plt.plot(param_values, test_scores, label='Test')
    plt.xlabel('k')
    plt.ylabel('AUC')
    plt.title('Learning Curve')
    plt.legend()
    plt.show()

param_values = [int(x) for x in np.linspace(1, 100, 100)]
draw_learning_curve(param_values)

"""## Model Experiment : kNN"""

df_ab = pd.read_csv('/content/gdrive/MyDrive/dataset/train_20D8GL3.csv')

df_ab.rename(columns = {'PAY_0': 'PAY_1'},inplace = True)

# Mengubah tipe data untuk kolom MARRIAGE
df_ab['MARRIAGE'] = df_ab['MARRIAGE'].replace(1, "Married")
df_ab['MARRIAGE'] = df_ab['MARRIAGE'].replace(2, "Single")
df_ab['MARRIAGE'] = df_ab['MARRIAGE'].replace(3, "Divorced")
df_ab['MARRIAGE'] = df_ab['MARRIAGE'].replace(0, "Unknown")

OHC = pd.get_dummies(df_ab['MARRIAGE'], prefix = 'status')
df_ab = df_ab.join(OHC)

for i in ['PAY_1', 'PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'] :
    OHC = pd.get_dummies(df_ab[i], prefix = i)
    df_ab = df_ab.join(OHC)

df_ab.head(2)

from sklearn.model_selection import train_test_split
train,test = train_test_split(df_ab, test_size=0.3, random_state=42)

from scipy import stats
# Menghapus outlier berdasarkan Z-score
filter = np.array([True] * len(train))

for x in nums:
    zscore = abs(stats.zscore(train[x]))
    filter = (zscore < 3) & filter

train_zscore = train[filter]
print('Jumlah baris sebelum outliers :',len(train))
print('Jumlah baris sesudah outliers :',len(train_zscore))
print('percent of outlier: ', train_zscore.shape[0]/train.shape[0] * 100, '%')

from scipy.sparse import issparse

from sklearn.preprocessing import MinMaxScaler, StandardScaler

from sklearn.preprocessing import RobustScaler

#Proses Scaling Data Train
train_zscore['age_norm'] = MinMaxScaler().fit_transform(train_zscore['AGE'].values.reshape(len(train_zscore), 1))

train_zscore['limitbal_norm'] = MinMaxScaler().fit_transform(train_zscore['LIMIT_BAL'].values.reshape(len(train_zscore), 1))

train_zscore['billamt1_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT1'].values.reshape(len(train_zscore), 1))

train_zscore['billamt2_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT2'].values.reshape(len(train_zscore), 1))

train_zscore['billamt3_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT3'].values.reshape(len(train_zscore), 1))

train_zscore['billamt4_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT4'].values.reshape(len(train_zscore), 1))

train_zscore['billamt5_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT5'].values.reshape(len(train_zscore), 1))

train_zscore['billamt6_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT6'].values.reshape(len(train_zscore), 1))

train_zscore['payamt1_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT1'].values.reshape(len(train_zscore), 1))

train_zscore['payamt2_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT2'].values.reshape(len(train_zscore), 1))

train_zscore['payamt3_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT3'].values.reshape(len(train_zscore), 1))

train_zscore['payamt4_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT4'].values.reshape(len(train_zscore), 1))

train_zscore['payamt5_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT5'].values.reshape(len(train_zscore), 1))

train_zscore['payamt6_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT6'].values.reshape(len(train_zscore), 1))

train = train_zscore

#Proses Scaling Data Test
test['age_norm'] = MinMaxScaler().fit_transform(test['AGE'].values.reshape(len(test), 1))

test['limitbal_norm'] = MinMaxScaler().fit_transform(test['LIMIT_BAL'].values.reshape(len(test), 1))

test['billamt1_std'] = StandardScaler().fit_transform(test['BILL_AMT1'].values.reshape(len(test), 1))

test['billamt2_std'] = StandardScaler().fit_transform(test['BILL_AMT2'].values.reshape(len(test), 1))

test['billamt3_std'] = StandardScaler().fit_transform(test['BILL_AMT3'].values.reshape(len(test), 1))

test['billamt4_std'] = StandardScaler().fit_transform(test['BILL_AMT4'].values.reshape(len(test), 1))

test['billamt5_std'] = StandardScaler().fit_transform(test['BILL_AMT5'].values.reshape(len(test), 1))

test['billamt6_std'] = StandardScaler().fit_transform(test['BILL_AMT6'].values.reshape(len(test), 1))

test['payamt1_std'] = StandardScaler().fit_transform(test['PAY_AMT1'].values.reshape(len(test), 1))

test['payamt2_std'] = StandardScaler().fit_transform(test['PAY_AMT2'].values.reshape(len(test), 1))

test['payamt3_std'] = StandardScaler().fit_transform(test['PAY_AMT3'].values.reshape(len(test), 1))

test['payamt4_std'] = StandardScaler().fit_transform(test['PAY_AMT4'].values.reshape(len(test), 1))

test['payamt5_std'] = StandardScaler().fit_transform(test['PAY_AMT5'].values.reshape(len(test), 1))

test['payamt6_std'] = StandardScaler().fit_transform(test['PAY_AMT6'].values.reshape(len(test), 1))

train = train.drop(columns = ['ID','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'MARRIAGE',
                                       'LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                                      'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])
test = test.drop(columns = ['ID','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'MARRIAGE',
                                       'LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                                      'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])

train.info()

test.info()

# mengelompokkan data train menjadi x & y
X_train = train.drop(['default_payment_next_month'],axis=1)
y_train = train['default_payment_next_month']

# mengelompokkan data test menjadi x & y
X_test = test.drop(['default_payment_next_month'],axis=1)
y_test = test['default_payment_next_month']

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_validate
from sklearn.metrics import confusion_matrix

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
def eval_classification(model):
    ypred_proba_test = model.predict_proba(X_test)
    ypred_proba_train = model.predict_proba(X_train)
    ypred_datatest = model.predict(X_test)
    ypred_datatrain = model.predict(X_train)

    cm = confusion_matrix(y_test, ypred_datatest)
    tn, fp, fn, tp = cm.ravel()

    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, ypred_datatest))
    print("Precision (Test Set): %.2f" % precision_score(y_test, ypred_datatest))
    print("Recall (Test Set): %.2f" % recall_score(y_test, ypred_datatest))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, ypred_datatest))
    print("ROC_AUC (test-proba): %.2f" % roc_auc_score(y_test, ypred_proba_test[:,1] ))
    print("ROC_AUC (train-proba): %.2f" % roc_auc_score(y_train, ypred_proba_train[:,1] ))
    print('TP  : {}\nFP  : {}\nFN  : {}\nTN  : {}\n'.format(tp, fp, fn, tn))

"""### Fit Model dengan Data Tanpa Preprocessing"""

# KNeighborsClassifierneighbors
from sklearn.neighbors import KNeighborsClassifier # import knn dari sklearn
knn = KNeighborsClassifier() # inisiasi object dengan nama knn
knn.fit(X_train, y_train) # fit model KNN dari data train
eval_classification(knn)

# Hyper tuning parameter with weight uniform and alg brute
from sklearn.model_selection import RandomizedSearchCV

n_neighbors = list(range(3,30,2))
p=[1,2]
Weight= ['uniform']
algorithm = ['brute']
hyperparameters = dict(n_neighbors=n_neighbors, p=p, algorithm=algorithm)

knn.fit(X_train, y_train)
rs_knn = RandomizedSearchCV(knn, hyperparameters, scoring='roc_auc', random_state=42)
rs_knn.fit(X_train, y_train)
eval_classification(rs_knn)

"""# Model Experiment: SVM"""

df_tanto = pd.read_csv('/content/gdrive/MyDrive/dataset/train_20D8GL3.csv')

df_tanto.rename(columns = {'PAY_0': 'PAY_1'},inplace = True)

# Mengubah tipe data untuk kolom MARRIAGE
df_tanto['MARRIAGE'] = df_tanto['MARRIAGE'].replace(1, "Married")
df_tanto['MARRIAGE'] = df_tanto['MARRIAGE'].replace(2, "Single")
df_tanto['MARRIAGE'] = df_tanto['MARRIAGE'].replace(3, "Divorced")
df_tanto['MARRIAGE'] = df_tanto['MARRIAGE'].replace(0, "Unknown")

OHC = pd.get_dummies(df_tanto['MARRIAGE'], prefix = 'status')
df_tanto = df_tanto.join(OHC)

for i in ['PAY_1', 'PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'] :
    OHC = pd.get_dummies(df_tanto[i], prefix = i)
    df_tanto = df_tanto.join(OHC)

from sklearn.model_selection import train_test_split
train,test = train_test_split(df_tanto, test_size=0.3, random_state=42)

from scipy import stats
# Menghapus outlier berdasarkan Z-score
filter = np.array([True] * len(train))

for x in nums:
    zscore = abs(stats.zscore(train[x]))
    filter = (zscore < 3) & filter

train_zscore = train[filter]
print('Jumlah baris sebelum outliers :',len(train))
print('Jumlah baris sesudah outliers :',len(train_zscore))
print('percent of outlier: ', train_zscore.shape[0]/train.shape[0] * 100, '%')

from scipy.sparse import issparse

from sklearn.preprocessing import MinMaxScaler, StandardScaler

from sklearn.preprocessing import RobustScaler

#Proses Scaling Data Train
train_zscore['age_norm'] = MinMaxScaler().fit_transform(train_zscore['AGE'].values.reshape(len(train_zscore), 1))

train_zscore['limitbal_norm'] = MinMaxScaler().fit_transform(train_zscore['LIMIT_BAL'].values.reshape(len(train_zscore), 1))

train_zscore['billamt1_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT1'].values.reshape(len(train_zscore), 1))

train_zscore['billamt2_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT2'].values.reshape(len(train_zscore), 1))

train_zscore['billamt3_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT3'].values.reshape(len(train_zscore), 1))

train_zscore['billamt4_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT4'].values.reshape(len(train_zscore), 1))

train_zscore['billamt5_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT5'].values.reshape(len(train_zscore), 1))

train_zscore['billamt6_std'] = StandardScaler().fit_transform(train_zscore['BILL_AMT6'].values.reshape(len(train_zscore), 1))

train_zscore['payamt1_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT1'].values.reshape(len(train_zscore), 1))

train_zscore['payamt2_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT2'].values.reshape(len(train_zscore), 1))

train_zscore['payamt3_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT3'].values.reshape(len(train_zscore), 1))

train_zscore['payamt4_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT4'].values.reshape(len(train_zscore), 1))

train_zscore['payamt5_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT5'].values.reshape(len(train_zscore), 1))

train_zscore['payamt6_std'] = StandardScaler().fit_transform(train_zscore['PAY_AMT6'].values.reshape(len(train_zscore), 1))

train = train_zscore

#Proses Scaling Data Test
test['age_norm'] = MinMaxScaler().fit_transform(test['AGE'].values.reshape(len(test), 1))

test['limitbal_norm'] = MinMaxScaler().fit_transform(test['LIMIT_BAL'].values.reshape(len(test), 1))

test['billamt1_std'] = StandardScaler().fit_transform(test['BILL_AMT1'].values.reshape(len(test), 1))

test['billamt2_std'] = StandardScaler().fit_transform(test['BILL_AMT2'].values.reshape(len(test), 1))

test['billamt3_std'] = StandardScaler().fit_transform(test['BILL_AMT3'].values.reshape(len(test), 1))

test['billamt4_std'] = StandardScaler().fit_transform(test['BILL_AMT4'].values.reshape(len(test), 1))

test['billamt5_std'] = StandardScaler().fit_transform(test['BILL_AMT5'].values.reshape(len(test), 1))

test['billamt6_std'] = StandardScaler().fit_transform(test['BILL_AMT6'].values.reshape(len(test), 1))

test['payamt1_std'] = StandardScaler().fit_transform(test['PAY_AMT1'].values.reshape(len(test), 1))

test['payamt2_std'] = StandardScaler().fit_transform(test['PAY_AMT2'].values.reshape(len(test), 1))

test['payamt3_std'] = StandardScaler().fit_transform(test['PAY_AMT3'].values.reshape(len(test), 1))

test['payamt4_std'] = StandardScaler().fit_transform(test['PAY_AMT4'].values.reshape(len(test), 1))

test['payamt5_std'] = StandardScaler().fit_transform(test['PAY_AMT5'].values.reshape(len(test), 1))

test['payamt6_std'] = StandardScaler().fit_transform(test['PAY_AMT6'].values.reshape(len(test), 1))

train = train.drop(columns = ['ID','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'MARRIAGE',
                                       'LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                                      'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])
test = test.drop(columns = ['ID','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'MARRIAGE',
                                       'LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                                      'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])

#Memisahkan x,y
x_train = train.drop(['default_payment_next_month'],axis=1)
y_train = train['default_payment_next_month']

x_test = test.drop(['default_payment_next_month'],axis=1)
y_test = test['default_payment_next_month']

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_validate

def eval_classification(model):
  y_pred = model.predict(x_test)
  y_pred_train = model.predict(x_train)
  y_pred_proba = model.predict_proba(x_test)
  y_pred_proba_train = model.predict_proba(x_train)

  print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
  print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
  print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
  print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))

  print("roc_auc (test-proba): %.2f" % roc_auc_score(y_test, y_pred_proba[:, 1]))
  print("roc_auc (train-proba): %.2f" % roc_auc_score(y_train, y_pred_proba_train[:, 1]))

def show_feature_importance(model):
  feat_importances = pd.Series(model.feature_importances_, index=x.columns)
  ax = feat_importances.nlargest(25).plot(kind='barh',figsize=(10,8))
  ax.invert_yaxis()

  plt.xlabel('score')
  plt.ylabel('feature')
  plt.title('feature importance score')

def show_best_hyperparameter(model):
  print(model.best_estimator_.get_params())

"""## Fit Model"""

from sklearn import svm
from sklearn.svm import SVC

clf = svm.SVC(probability=True)
clf.fit(x_train, y_train)

eval_classification(clf)

"""## Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC


C = [0.1, 1, 10, 100, 1000]
gamma = [1, 0.1, 0.01, 0.001, 0.0001]
kernel = ['rbf']

hyperparameters = dict(C=C, gamma=gamma, kernel=kernel)
grid = RandomizedSearchCV(clf, hyperparameters, refit = True, verbose = 3, random_state=42)
grid.fit(x_train, y_train)
eval_classification(grid)

"""<h1><b>XGBoost<b></h1>"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
def eval_classification(model):
    ypred_proba_test = model.predict_proba(x_test)
    ypred_proba_train = model.predict_proba(x_train)
    ypred_datatest = model.predict(x_test)
    ypred_datatrain = model.predict(x_train)

    cm = confusion_matrix(y_test, ypred_datatest)
    tn, fp, fn, tp = cm.ravel()

    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, ypred_datatest))
    print("Precision (Test Set): %.2f" % precision_score(y_test, ypred_datatest))
    print("Recall (Test Set): %.2f" % recall_score(y_test, ypred_datatest))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, ypred_datatest))
    print("AUC (test-proba): %.2f" % roc_auc_score(y_test, ypred_proba_test[:,1] ))
    print("AUC (train-proba): %.2f" % roc_auc_score(y_train, ypred_proba_train[:,1] ))
    print('TP  : {}\nFP  : {}\nFN  : {}\nTN  : {}\n'.format(tp, fp, fn, tn))

"""### Fit Model"""

from xgboost import XGBClassifier

xg = XGBClassifier()
xg.fit(x_train, y_train)
eval_classification(xg)

"""##Hyperparameter Tuning

Memakai semua paramater untuk mencari hyperparameter yang terbaik
"""

#Hyperparameter Tuning

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

#Menjadikan ke dalam bentuk dictionary
hyperparameters = {
                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'scale_pos_weight' : [float(x) for x in np.linspace(0, 1, num = 11)]
                    }

# Init
from xgboost import XGBClassifier
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, random_state=42, scoring='roc_auc')
xg_tuned.fit(x_train,y_train)

# Predict & Evaluation
eval_classification(xg_tuned)

"""<h2><b>Hyperparameter Tuning (BEST)</b></h2>"""

#Hyperparameter Tuning (BEST)

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

#Menjadikan ke dalam bentuk dictionary
hyperparameters = {
                    'learning_rate':[0.05, 0.1, 0.15],
                    'max_depth': list(range(3, 21, 3)),
                    'n_estimators': list(range(60, 160, 20))
                    }


# Init
from xgboost import XGBClassifier
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, random_state=42, scoring='roc_auc')
xg_tuned.fit(x_train,y_train)

# Predict & Evaluation
eval_classification(xg_tuned)

"""## Feature Importance"""

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=x_train.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

show_feature_importance(xg)

"""# Random Forest"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import cross_validate

def eval_classification(model):
    y_pred = model.predict(x_test)
    y_pred_train = model.predict(x_train)
    y_pred_proba = model.predict_proba(x_test)
    y_pred_proba_train = model.predict_proba(x_train)
    y_pred_datatest = model.predict(x_test)

    cm = confusion_matrix(y_test, y_pred_datatest)
    tn, fp, fn, tp = cm.ravel()

    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
    print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
    print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))

    print("roc_auc (test-proba): %.2f" % roc_auc_score(y_test, y_pred_proba[:, 1]))
    print("roc_auc (train-proba): %.2f" % roc_auc_score(y_train, y_pred_proba_train[:, 1]))

    print('TP  : {}\nFP  : {}\nFN  : {}\nTN  : {}\n'.format(tp, fp, fn, tn))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)
rf.fit(x_train, y_train)
eval_classification(rf)

"""##Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(1, 200, 50)]
criterion = ['gini', 'entropy']
max_depth = [int(x) for x in np.linspace(2, 100, 50)]
min_samples_split = [int(x) for x in np.linspace(2, 20, 10)]
min_samples_leaf = [int(x) for x in np.linspace(2, 20, 10)]
hyperparameters = dict(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,
                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)

rf = RandomForestClassifier(random_state=42)
rs = RandomizedSearchCV(rf, hyperparameters, scoring='roc_auc', random_state=1, cv=5)
rs.fit(x_train, y_train)
eval_classification(rs)

"""# Decision Tree"""

# Categorizing data based on their data type except for ID column and default_payment_next_month column

# Categorical data
categorical = ['SEX','EDUCATION','MARRIAGE']
pay_delay = ['PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']

# Numerical data
numerical = ['LIMIT_BAL', 'AGE',
             'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',
             'PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']

numerical1 = ['LIMIT_BAL','AGE']

bill_amt = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']
pay_amt = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']

df = pd.read_csv('/content/gdrive/MyDrive/dataset/train_20D8GL3.csv')

df13 = df

df13.head(2)

df13.rename(columns = {'PAY_0': 'PAY_1'},inplace = True)

df13["MARRIAGE"] = df13["MARRIAGE"].replace(0, "UNKNOWN")
df13["MARRIAGE"] = df13["MARRIAGE"].replace(1, "MARRIED")
df13["MARRIAGE"] = df13["MARRIAGE"].replace(2, "SINGLE")
df13["MARRIAGE"] = df13["MARRIAGE"].replace(3, "DIVORCED")

OHE = pd.get_dummies(df13['MARRIAGE'], prefix = 'MARRIAGE_STATUS')
df13 = df13.join(OHE)

for i in ['PAY_1', 'PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'] :
    OHE1 = pd.get_dummies(df13[i], prefix = i)
    df13 = df13.join(OHE1)

# Splitting dataset into 70% training data and 30% testing data
from sklearn.model_selection import train_test_split
df13_train,df13_test = train_test_split(df13, test_size=0.3, random_state=42)

print(f"Dimensi df13_train : {df13_train.shape}\nDimensi df13_test  : {df13_test.shape}")

from scipy import stats

filter = np.array([True] * len(df13_train))

for i in numerical:
  zscore = np.abs(stats.zscore(df13_train[i]))
  filter = (zscore < 3) & filter

df13_train_zscore = df13_train[filter]

df13_train_filtered1 = round(len(df13_train_zscore)/len(df13_train)*100,2)

print("Z-Score")
print(f"Jumlah baris sebelum handling outliers: {len(df13_train)}")
print(f"Jumlah baris sesudah handling outliers: {len(df13_train_zscore)}")
print()
print(f"Banyak baris yang tersisa setelah handling outliers: {df13_train_filtered1}%")
print(f"Banyak outlier yang terhapus: {round(100 - df13_train_filtered1,2)}% ")

df13_train = df13_train_zscore
df13_train.shape

from scipy.sparse import issparse

from sklearn.preprocessing import StandardScaler

# Scaling Training Dataset
df13_train['AGE_STD'] = StandardScaler().fit_transform(df13_train['AGE'].values.reshape(len(df13_train), 1))

df13_train['LIMITBAL_STD'] = StandardScaler().fit_transform(df13_train['LIMIT_BAL'].values.reshape(len(df13_train), 1))

df13_train['BILLAMT1_STD'] = StandardScaler().fit_transform(df13_train['BILL_AMT1'].values.reshape(len(df13_train), 1))

df13_train['BILLAMT2_STD'] = StandardScaler().fit_transform(df13_train['BILL_AMT2'].values.reshape(len(df13_train), 1))

df13_train['BILLAMT3_STD'] = StandardScaler().fit_transform(df13_train['BILL_AMT3'].values.reshape(len(df13_train), 1))

df13_train['BILLAMT4_STD'] = StandardScaler().fit_transform(df13_train['BILL_AMT4'].values.reshape(len(df13_train), 1))

df13_train['BILLAMT5_STD'] = StandardScaler().fit_transform(df13_train['BILL_AMT5'].values.reshape(len(df13_train), 1))

df13_train['BILLAMT6_STD'] = StandardScaler().fit_transform(df13_train['BILL_AMT6'].values.reshape(len(df13_train), 1))

df13_train['PAYAMT1_STD'] = StandardScaler().fit_transform(df13_train['PAY_AMT1'].values.reshape(len(df13_train), 1))

df13_train['PAYAMT2_STD'] = StandardScaler().fit_transform(df13_train['PAY_AMT2'].values.reshape(len(df13_train), 1))

df13_train['PAYAMT3_STD'] = StandardScaler().fit_transform(df13_train['PAY_AMT3'].values.reshape(len(df13_train), 1))

df13_train['PAYAMT4_STD'] = StandardScaler().fit_transform(df13_train['PAY_AMT4'].values.reshape(len(df13_train), 1))

df13_train['PAYAMT5_STD'] = StandardScaler().fit_transform(df13_train['PAY_AMT5'].values.reshape(len(df13_train), 1))

df13_train['PAYAMT6_STD'] = StandardScaler().fit_transform(df13_train['PAY_AMT6'].values.reshape(len(df13_train), 1))

df13_train = df13_train.drop(columns = ['ID','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'MARRIAGE',
                                       'LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                                      'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])

df13_train.shape

# Scaling Testing Dataset
df13_test['AGE_STD'] = StandardScaler().fit_transform(df13_test['AGE'].values.reshape(len(df13_test), 1))

df13_test['LIMITBAL_STD'] = StandardScaler().fit_transform(df13_test['LIMIT_BAL'].values.reshape(len(df13_test), 1))

df13_test['BILLAMT1_STD'] = StandardScaler().fit_transform(df13_test['BILL_AMT1'].values.reshape(len(df13_test), 1))

df13_test['BILLAMT2_STD'] = StandardScaler().fit_transform(df13_test['BILL_AMT2'].values.reshape(len(df13_test), 1))

df13_test['BILLAMT3_STD'] = StandardScaler().fit_transform(df13_test['BILL_AMT3'].values.reshape(len(df13_test), 1))

df13_test['BILLAMT4_STD'] = StandardScaler().fit_transform(df13_test['BILL_AMT4'].values.reshape(len(df13_test), 1))

df13_test['BILLAMT5_STD'] = StandardScaler().fit_transform(df13_test['BILL_AMT5'].values.reshape(len(df13_test), 1))

df13_test['BILLAMT6_STD'] = StandardScaler().fit_transform(df13_test['BILL_AMT6'].values.reshape(len(df13_test), 1))

df13_test['PAYAMT1_STD'] = StandardScaler().fit_transform(df13_test['PAY_AMT1'].values.reshape(len(df13_test), 1))

df13_test['PAYAMT2_STD'] = StandardScaler().fit_transform(df13_test['PAY_AMT2'].values.reshape(len(df13_test), 1))

df13_test['PAYAMT3_STD'] = StandardScaler().fit_transform(df13_test['PAY_AMT3'].values.reshape(len(df13_test), 1))

df13_test['PAYAMT4_STD'] = StandardScaler().fit_transform(df13_test['PAY_AMT4'].values.reshape(len(df13_test), 1))

df13_test['PAYAMT5_STD'] = StandardScaler().fit_transform(df13_test['PAY_AMT5'].values.reshape(len(df13_test), 1))

df13_test['PAYAMT6_STD'] = StandardScaler().fit_transform(df13_test['PAY_AMT6'].values.reshape(len(df13_test), 1))

df13_test = df13_test.drop(columns = ['ID','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'MARRIAGE',
                                       'LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                                      'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])

df13_test.shape

# Splitting dataset into X and Y
x13_train = df13_train.drop(['default_payment_next_month'],axis=1)
y13_train = df13_train['default_payment_next_month']

print(f"Dimensi x13_train : {x13_train.shape}\nDimensi y13_train  : {y13_train.shape}")

x13_test = df13_test.drop(['default_payment_next_month'],axis=1)
y13_test = df13_test['default_payment_next_month']

print(f"Dimensi x13_test : {x13_test.shape}\nDimensi y13_test  : {y13_test.shape}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import cross_validate

def eval_classification(model):
    y13_pred = model.predict(x13_test)
    y13_pred_train = model.predict(x13_train)
    y13_pred_proba = model.predict_proba(x13_test)
    y13_pred_proba_train = model.predict_proba(x13_train)
    y13_pred_datatest = model.predict(x13_test)

    cm = confusion_matrix(y13_test,y13_pred_datatest)
    tn, fp, fn, tp = cm.ravel()

    print("Accuracy (Test Set): %.2f" % accuracy_score(y13_test,y13_pred))
    print("Precision (Test Set): %.2f" % precision_score(y13_test,y13_pred))
    print("Recall (Test Set): %.2f" % recall_score(y13_test,y13_pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y13_test,y13_pred))

    print("roc_auc (test-proba): %.2f" % roc_auc_score(y13_test,y13_pred_proba[:, 1]))
    print("roc_auc (train-proba): %.2f" % roc_auc_score(y13_train,y13_pred_proba_train[:, 1]))\

    print()
    print('TP  : {}\nFP  : {}\nFN  : {}\nTN  : {}\n'.format(tp, fp, fn, tn))

X13 = x13_train

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X13.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

"""### FIT MODEL"""

from sklearn.tree import DecisionTreeClassifier # import decision tree dari sklearn
dt13 = DecisionTreeClassifier() # inisiasi object dengan nama dt13
dt13.fit(x13_train, y13_train) # fit model decision tree dari data train

print('\033[1mDecision Tree Evaluation\033[0m\n')
eval_classification(dt13)

"""### Hyperparameter Tunning"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import uniform
import numpy as np

# List of hyperparameter
max_depth = [int(x) for x in np.linspace(1, 10, 10)] # Maximum number of levels in tree

# min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_split = [int(x) for x in np.linspace(2, 10000, 50)]
# min_samples_split = [int(x) for x in np.linspace(2, 9000, 50)]

# min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
min_samples_leaf = [int(x) for x in np.linspace(2, 2000, 50)]

max_features = ['auto', 'sqrt'] # Number of features to consider at every split
criterion = ['gini','entropy']
splitter = ['best','random']

hyperparameters = dict(max_depth=max_depth,
                       min_samples_split=min_samples_split,
                       min_samples_leaf=min_samples_leaf,
                       max_features=max_features,
                       criterion=criterion,
                       splitter=splitter
                      )

# Inisialisasi Model
dt13 = DecisionTreeClassifier(random_state=42)
rs_dt13 = RandomizedSearchCV(dt13, hyperparameters,scoring='roc_auc')
rs_dt13.fit(x13_train, y13_train)

# Predict & Evaluation
y13_pred = rs_dt13.predict(x13_test) #Check performa dari model

print('\033[1mDecision Tree Evaluation - Hyperparameter Tuning\033[0m\n')
eval_classification(rs_dt13)

show_best_hyperparameter(rs_dt13)

show_feature_importance(rs_dt13.best_estimator_)

